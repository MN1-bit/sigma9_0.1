# ============================================================================
# Parquet Manager - Parquet íŒŒì¼ Read/Write ê´€ë¦¬
# ============================================================================
# ğŸ“Œ ì´ íŒŒì¼ì˜ ì—­í• :
#   - Parquet í¬ë§·ìœ¼ë¡œ ì‹œì¥ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ
#   - í‹°ì»¤ë³„ ë¶„ë´‰ íŒŒì¼ (AAPL_1m.parquet, AAPL_1h.parquet)
#   - ì „ì²´ ì¼ë´‰ í†µí•© íŒŒì¼ (daily_all.parquet)
#   - SQLite ëŒ€ë¹„ ì»¬ëŸ¼í˜• ì €ì¥ì†Œë¡œ ë¶„ì„ ì¿¼ë¦¬ ìµœì í™”
#
# ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
#   >>> pm = ParquetManager("data/parquet")
#   >>> df = pd.DataFrame([{"ticker": "AAPL", "date": "2024-01-01", ...}])
#   >>> pm.append_daily(df)
#   >>> result = pm.read_daily("AAPL", days=30)
# ============================================================================

from pathlib import Path
from typing import Callable, Optional
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from loguru import logger
from datetime import datetime, timedelta


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¦¬ìƒ˜í”Œë§ ê·œì¹™ ìƒìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ELI5: íŒŒìƒ íƒ€ì„í”„ë ˆì„ì„ ì–´ë–¤ ì†ŒìŠ¤ì—ì„œ ìƒì„±í• ì§€ ì •ì˜í•©ë‹ˆë‹¤.
#       ì˜ˆ: 5ë¶„ë´‰ = 1ë¶„ë´‰ 5ê°œì˜ OHLCVë¥¼ ì§‘ê³„
RESAMPLE_RULES: dict[str, tuple[str, str]] = {
    "3m": ("1m", "3min"),   # 1ë¶„ë´‰ 3ê°œ â†’ 3ë¶„ë´‰
    "5m": ("1m", "5min"),   # 1ë¶„ë´‰ 5ê°œ â†’ 5ë¶„ë´‰
    "15m": ("1m", "15min"), # 1ë¶„ë´‰ 15ê°œ â†’ 15ë¶„ë´‰
    "4h": ("1h", "4h"),     # 1ì‹œê°„ë´‰ 4ê°œ â†’ 4ì‹œê°„ë´‰
    "1W": ("1D", "W-FRI"),  # ì¼ë´‰ 5ê°œ â†’ ì£¼ë´‰ (ê¸ˆìš”ì¼ ê¸°ì¤€)
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ParquetManager í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ParquetManager:
    """
    Parquet íŒŒì¼ Read/Write ê´€ë¦¬ì

    SQLiteì˜ DailyBar/IntradayBar ë°ì´í„°ë¥¼ Parquet í¬ë§·ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    ì»¬ëŸ¼í˜• ì €ì¥ì†Œ íŠ¹ì„±ìƒ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        base_dir: Parquet íŒŒì¼ ì €ì¥ ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬
        intraday_dir: ë¶„ë´‰ íŒŒì¼ ì €ì¥ ë””ë ‰í„°ë¦¬ (í‹°ì»¤ë³„ ë¶„ë¦¬)
        daily_path: ì¼ë´‰ í†µí•© íŒŒì¼ ê²½ë¡œ

    Example:
        >>> pm = ParquetManager("data/parquet")
        >>> df = pd.DataFrame([{"ticker": "AAPL", "date": "2024-01-01", ...}])
        >>> pm.append_daily(df)
        >>> result = pm.read_daily("AAPL", days=30)
    """

    # [11-003] ì§€ì›í•˜ëŠ” íƒ€ì„í”„ë ˆì„ ëª©ë¡
    SUPPORTED_TIMEFRAMES: list[str] = ["1m", "3m", "5m", "15m", "1h", "4h"]

    def __init__(self, base_dir: str = "data/parquet"):
        """
        ParquetManager ì´ˆê¸°í™”

        [11-003] íƒ€ì„í”„ë ˆì„ë³„ í´ë” êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        ê¸°ì¡´: intraday/AAPL_1m.parquet
        ì‹ ê·œ: 1m/AAPL.parquet

        Args:
            base_dir: Parquet íŒŒì¼ ì €ì¥ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
        """
        # ê²½ë¡œ ì„¤ì • (ELI5: íŒŒì¼ì„ ì €ì¥í•  í´ë” ìœ„ì¹˜ë¥¼ ì •í•©ë‹ˆë‹¤)
        self.base_dir = Path(base_dir)
        self.daily_path = self.base_dir / "daily" / "all_daily.parquet"

        # [11-003] íƒ€ì„í”„ë ˆì„ë³„ ë””ë ‰í„°ë¦¬ (ELI5: 1m/, 5m/, 1h/ í´ë”ë¡œ ë¶„ë¦¬)
        self._tf_dirs: dict[str, Path] = {}
        for tf in self.SUPPORTED_TIMEFRAMES:
            tf_dir = self.base_dir / tf
            tf_dir.mkdir(parents=True, exist_ok=True)
            self._tf_dirs[tf] = tf_dir

        # Daily ë””ë ‰í„°ë¦¬ ìƒì„±
        self.daily_path.parent.mkdir(parents=True, exist_ok=True)

        # [11-003] ë ˆê±°ì‹œ intraday í´ë” (í•˜ìœ„ í˜¸í™˜ì„± - ì½ê¸° ì „ìš© fallback)
        self._legacy_intraday_dir = self.base_dir / "intraday"

        logger.info(f"ğŸ“¦ ParquetManager initialized: {self.base_dir}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Daily (ì¼ë´‰) - ì „ì²´ í‹°ì»¤ í†µí•© íŒŒì¼
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def write_daily(self, df: pd.DataFrame) -> int:
        """
        ì¼ë´‰ ë°ì´í„° ì „ì²´ ë®ì–´ì“°ê¸° (ì´ˆê¸° ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)

        [12-002] í‹°ì»¤ ì •ë ¬ + Row Group í¬ê¸° ì„¤ì •ìœ¼ë¡œ Predicate Pushdown ìµœì í™”

        Args:
            df: ì €ì¥í•  DataFrame (ticker, date, open, high, low, close, volume í•„ìˆ˜)

        Returns:
            int: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if df.empty:
            return 0

        # [12-002] ë°ì´í„° ì •ë ¬ (í‹°ì»¤, ë‚ ì§œ ìˆœ) - Row Group ë‚´ í‹°ì»¤ ì—°ì† ë°°ì¹˜
        # ELI5: ê°™ì€ í‹°ì»¤ë¼ë¦¬ ëª¨ì•„ë‘ë©´ íŠ¹ì • í‹°ì»¤ë§Œ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        df = df.sort_values(["ticker", "date"]).reset_index(drop=True)

        # [12-002] Row Group í¬ê¸° ì„¤ì • (50ë§Œ í–‰) - Predicate Pushdown íš¨ìœ¨í™”
        # ELI5: íŒŒì¼ì„ ì‘ì€ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ ì„œ í•„ìš”í•œ ë¸”ë¡ë§Œ ì½ìŠµë‹ˆë‹¤
        pq.write_table(
            pa.Table.from_pandas(df),
            self.daily_path,
            compression="snappy",
            row_group_size=500_000,  # 50ë§Œ í–‰ = ~25-30 Row Groups
        )

        logger.info(f"ğŸ“ Daily written: {len(df)} rows â†’ {self.daily_path}")
        return len(df)

    def append_daily(self, df: pd.DataFrame) -> int:
        """
        ì¼ë´‰ ë°ì´í„° ì¶”ê°€ (ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)

        ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ë©°, ì¤‘ë³µ ì œê±° (ticker + date ê¸°ì¤€)

        Args:
            df: ì¶”ê°€í•  DataFrame

        Returns:
            int: ìµœì¢… ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if df.empty:
            return 0

        # ê¸°ì¡´ ë°ì´í„° ì½ê¸° (ELI5: ì´ë¯¸ ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤)
        if self.daily_path.exists():
            existing = pq.read_table(self.daily_path).to_pandas()
            # ë³‘í•© í›„ ì¤‘ë³µ ì œê±° (ELI5: ê°™ì€ ë‚ ì§œì˜ ë°ì´í„°ëŠ” ìµœì‹  ê²ƒë§Œ ë‚¨ê¹ë‹ˆë‹¤)
            combined = pd.concat([existing, df], ignore_index=True)
            combined = combined.drop_duplicates(subset=["ticker", "date"], keep="last")
        else:
            combined = df

        return self.write_daily(combined)

    def read_daily(
        self,
        ticker: Optional[str] = None,
        days: Optional[int] = None,
        start_date: Optional[str] = None,
    ) -> pd.DataFrame:
        """
        ì¼ë´‰ ë°ì´í„° ì¡°íšŒ

        [12-002] Predicate Pushdown ì ìš© - í‹°ì»¤ ì§€ì • ì‹œ Row Group ë ˆë²¨ í•„í„°ë§

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼ (Noneì´ë©´ ì „ì²´)
            days: ì¡°íšŒí•  ì¼ìˆ˜ (None ë˜ëŠ” 0ì´ë©´ ì „ì²´ ì¡°íšŒ)
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            pd.DataFrame: ì¡°íšŒëœ ë°ì´í„° (ë¹ˆ ê²½ìš° ë¹ˆ DataFrame)
        """
        if not self.daily_path.exists():
            return pd.DataFrame()

        # [12-002] Predicate Pushdown - Row Group ë ˆë²¨ì—ì„œ í•„í„°ë§
        # ELI5: í‹°ì»¤ê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ í‹°ì»¤ê°€ ìˆëŠ” ë¸”ë¡ë§Œ ì½ì–´ì„œ ë¹ ë¦…ë‹ˆë‹¤
        if ticker:
            # PyArrow filters: Row Group í†µê³„ë¡œ í•„ìš”í•œ ë¸”ë¡ë§Œ ë¡œë“œ
            filters = [("ticker", "=", ticker)]
            df = pq.read_table(self.daily_path, filters=filters).to_pandas()
        else:
            # ì „ì²´ ì¡°íšŒ
            df = pq.read_table(self.daily_path).to_pandas()

        if df.empty:
            return df

        # ë‚ ì§œ í•„í„° (ELI5: ìµœê·¼ Nì¼ ë°ì´í„°ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤)
        if start_date:
            df = df[df["date"] >= start_date]
        elif days and days > 0:
            # ë°ì´í„° ë‚´ ìƒìœ„ Nê°œ ë‚ ì§œë§Œ ì¶”ì¶œ
            if not df.empty:
                unique_dates = sorted(df["date"].unique(), reverse=True)[:days]
                df = df[df["date"].isin(unique_dates)]

        return df.sort_values(["ticker", "date"]).reset_index(drop=True)

    def read_daily_bulk(
        self,
        tickers: list[str] | None = None,
        days: int = 20,
    ) -> dict[str, list[dict]]:
        """
        [12-002] ì—¬ëŸ¬ í‹°ì»¤ì˜ ì¼ë´‰ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ

        ELI5: íŒŒì¼ì„ 1íšŒë§Œ ì½ê³  í‹°ì»¤ë³„ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤.
              í‹°ì»¤ 10,000ê°œë¥¼ ì¡°íšŒí•˜ë”ë¼ë„ íŒŒì¼ ì½ê¸°ëŠ” 1ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

        ê¸°ì¡´ read_daily()ì™€ì˜ ì°¨ì´:
        - read_daily(ticker): í‹°ì»¤ë§ˆë‹¤ íŒŒì¼ ì½ê¸° â†’ O(N) I/O
        - read_daily_bulk(tickers): íŒŒì¼ 1íšŒ ì½ê¸° â†’ O(1) I/O

        Args:
            tickers: ì¡°íšŒí•  í‹°ì»¤ ëª©ë¡ (Noneì´ë©´ ì „ì²´ í‹°ì»¤)
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 20)

        Returns:
            dict[str, list[dict]]: í‹°ì»¤ â†’ ì¼ë´‰ ë°ì´í„° (ì˜¤ë˜ëœ ìˆœ ì •ë ¬)
                ì˜ˆ: {"AAPL": [{"date": "2024-01-01", ...}, ...], ...}
        """
        if not self.daily_path.exists():
            return {}

        # Step 1: íŒŒì¼ ì „ì²´ ì½ê¸° (ELI5: ì±… í•œ ê¶Œ ì „ì²´ë¥¼ í•œ ë²ˆì— ì½ìŠµë‹ˆë‹¤)
        df = pq.read_table(self.daily_path).to_pandas()

        if df.empty:
            return {}

        # Step 2: ë‚ ì§œ í•„í„°ë§ (ELI5: ìµœê·¼ Nì¼ì¹˜ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤)
        # ì „ì²´ ë°ì´í„°ì—ì„œ unique ë‚ ì§œ ì¶”ì¶œ â†’ ìµœì‹  Nê°œ ì„ íƒ
        unique_dates = sorted(df["date"].unique(), reverse=True)[:days]
        df = df[df["date"].isin(unique_dates)]

        # Step 3: í‹°ì»¤ í•„í„°ë§ (ELI5: ìš”ì²­í•œ í‹°ì»¤ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤)
        if tickers:
            df = df[df["ticker"].isin(tickers)]

        # Step 4: í‹°ì»¤ë³„ ê·¸ë£¹í™” (ELI5: í‹°ì»¤ë§ˆë‹¤ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜)
        # ê° í‹°ì»¤ì˜ ë°ì´í„°ë¥¼ ë‚ ì§œìˆœ ì •ë ¬ í›„ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result = {}
        for ticker, group in df.groupby("ticker"):
            sorted_data = group.sort_values("date").to_dict("records")
            result[ticker] = sorted_data

        return result

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Intraday (ë¶„ë´‰/ì‹œë´‰) - í‹°ì»¤ë³„ ë¶„ë¦¬ íŒŒì¼
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _get_intraday_path(self, ticker: str, timeframe: str) -> Path:
        """
        Intraday íŒŒì¼ ê²½ë¡œ ìƒì„±

        [11-003] íƒ€ì„í”„ë ˆì„ë³„ í´ë” êµ¬ì¡° (TFë³„ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”)
        ê²½ë¡œ í˜•ì‹: {base_dir}/{timeframe}/{ticker}.parquet
        ì˜ˆ: data/parquet/1m/AAPL.parquet

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: "AAPL")
            timeframe: íƒ€ì„í”„ë ˆì„ ("1m", "5m", "15m", "1h")

        Returns:
            Path: íŒŒì¼ ê²½ë¡œ (ì˜ˆ: data/parquet/1m/AAPL.parquet)
        """
        # ì§€ì›í•˜ëŠ” TFëŠ” ë¯¸ë¦¬ ìƒì„±ëœ í´ë” ì‚¬ìš©
        if timeframe in self._tf_dirs:
            return self._tf_dirs[timeframe] / f"{ticker}.parquet"

        # ë™ì  TFëŠ” í´ë” ìë™ ìƒì„± (ì˜ˆ: 30m)
        tf_dir = self.base_dir / timeframe
        tf_dir.mkdir(parents=True, exist_ok=True)
        return tf_dir / f"{ticker}.parquet"

    def write_intraday(self, ticker: str, timeframe: str, df: pd.DataFrame) -> int:
        """
        Intraday ë°ì´í„° ì „ì²´ ë®ì–´ì“°ê¸°

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„ ("1m", "5m", "15m", "1h")
            df: ì €ì¥í•  DataFrame

        Returns:
            int: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if df.empty:
            return 0

        path = self._get_intraday_path(ticker, timeframe)

        # ì‹œê°„ìˆœ ì •ë ¬ (ELI5: ì˜¤ë˜ëœ ë°ì´í„°ë¶€í„° ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬)
        df = df.sort_values("timestamp").reset_index(drop=True)

        pq.write_table(
            pa.Table.from_pandas(df),
            path,
            compression="snappy",
        )

        logger.debug(f"ğŸ“ Intraday written: {ticker}_{timeframe} â†’ {len(df)} rows")
        return len(df)

    def append_intraday(self, ticker: str, timeframe: str, df: pd.DataFrame) -> int:
        """
        Intraday ë°ì´í„° ì¶”ê°€ (ì¦ë¶„ ì—…ë°ì´íŠ¸)

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            df: ì¶”ê°€í•  DataFrame

        Returns:
            int: ìµœì¢… ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if df.empty:
            return 0

        path = self._get_intraday_path(ticker, timeframe)

        if path.exists():
            existing = pq.read_table(path).to_pandas()
            combined = pd.concat([existing, df], ignore_index=True)
            # timestamp ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            combined = combined.drop_duplicates(subset=["timestamp"], keep="last")
        else:
            combined = df

        return self.write_intraday(ticker, timeframe, combined)

    def read_intraday(
        self,
        ticker: str,
        timeframe: str,
        days: int = 2,
        start_timestamp: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        Intraday ë°ì´í„° ì¡°íšŒ

        [11-003] ìƒˆ êµ¬ì¡° ìš°ì„ , ë ˆê±°ì‹œ fallback ì§€ì›

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„ ("1m", "5m", "15m", "1h")
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 2ì¼)
            start_timestamp: ì‹œì‘ Unix timestamp (ë°€ë¦¬ì´ˆ)

        Returns:
            pd.DataFrame: ì¡°íšŒëœ ë°ì´í„°
        """
        path = self._get_intraday_path(ticker, timeframe)

        # [11-003] ìƒˆ êµ¬ì¡°ì— íŒŒì¼ì´ ì—†ìœ¼ë©´ ë ˆê±°ì‹œ ê²½ë¡œì—ì„œ ì‹œë„
        if not path.exists():
            legacy_path = self._legacy_intraday_dir / f"{ticker}_{timeframe}.parquet"
            if legacy_path.exists():
                logger.debug(f"[11-003] ë ˆê±°ì‹œ ê²½ë¡œì—ì„œ ì½ê¸°: {legacy_path}")
                path = legacy_path
            else:
                return pd.DataFrame()

        df = pq.read_table(path).to_pandas()

        if df.empty:
            return df

        # ì‹œê°„ í•„í„° (ELI5: ìµœê·¼ Nì¼ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤)
        if start_timestamp:
            df = df[df["timestamp"] >= start_timestamp]
        else:
            cutoff_ts = int((datetime.now() - timedelta(days=days)).timestamp() * 1000)
            df = df[df["timestamp"] >= cutoff_ts]

        return df.sort_values("timestamp").reset_index(drop=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # On-demand ë¦¬ìƒ˜í”Œë§ (09-002)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_intraday_bars(
        self,
        ticker: str,
        tf: str,
        auto_fill: bool = True,
        days: int = 14,
    ) -> pd.DataFrame:
        """
        Intraday ë°ì´í„° ì¡°íšŒ (On-demand ë¦¬ìƒ˜í”Œë§ ì§€ì›)

        ELI5: ìš”ì²­í•œ íƒ€ì„í”„ë ˆì„ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì†ŒìŠ¤ì—ì„œ ìë™ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
              ì˜ˆ) 5m íŒŒì¼ì´ ì—†ìœ¼ë©´ â†’ 1m íŒŒì¼ì—ì„œ 5ë¶„ë´‰ ìƒì„± â†’ ì €ì¥ â†’ ë°˜í™˜

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: "AAPL")
            tf: íƒ€ì„í”„ë ˆì„ ("1m", "3m", "5m", "15m", "1h", "4h", "1W")
            auto_fill: Trueë©´ íŒŒì¼ ì—†ì„ ë•Œ ìë™ ë¦¬ìƒ˜í”Œë§ ì‹œë„
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 14ì¼)

        Returns:
            pd.DataFrame: OHLCV ë°ì´í„° (ë¹ˆ ê²½ìš° ë¹ˆ DataFrame)
        """
        path = self._get_intraday_path(ticker, tf)

        if not path.exists():
            if auto_fill and tf in RESAMPLE_RULES:
                logger.warning(f"[GAP-FILL] {ticker}/{tf} íŒŒì¼ ì—†ìŒ, ë¦¬ìƒ˜í”Œë§ ì‹œë„")
                return self._try_resample(ticker, tf)
            return pd.DataFrame()

        return self.read_intraday(ticker, tf, days=days)

    def _try_resample(self, ticker: str, tf: str) -> pd.DataFrame:
        """
        ì†ŒìŠ¤ íƒ€ì„í”„ë ˆì„ì—ì„œ íƒ€ê²Ÿ íƒ€ì„í”„ë ˆì„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ ì‹œë„

        ELI5: 1ë¶„ë´‰ì´ ìˆìœ¼ë©´ 5ë¶„ë´‰ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤ (1ë¶„ë´‰ 5ê°œë¥¼ í•©ì³ì„œ).

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            tf: íƒ€ê²Ÿ íƒ€ì„í”„ë ˆì„ (ì˜ˆ: "5m")

        Returns:
            pd.DataFrame: ë¦¬ìƒ˜í”Œë§ëœ ë°ì´í„° (ì‹¤íŒ¨ ì‹œ ë¹ˆ DataFrame)
        """
        rule = RESAMPLE_RULES.get(tf)
        if not rule:
            logger.error(f"[GAP-FILL] {tf}ì— ëŒ€í•œ ë¦¬ìƒ˜í”Œ ê·œì¹™ ì—†ìŒ")
            return pd.DataFrame()

        source_tf, pandas_rule = rule

        # ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ (auto_fill=Falseë¡œ ì¬ê·€ ë°©ì§€)
        source_df = self.read_intraday(ticker, source_tf, days=30)
        if source_df.empty:
            logger.error(f"[GAP-FILL] {ticker}/{source_tf} ì†ŒìŠ¤ ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame()

        logger.info(f"[RESAMPLE] {ticker} {source_tf}â†’{tf} ì‹œì‘ ({len(source_df)} bars)")

        # ë¦¬ìƒ˜í”Œë§ ìˆ˜í–‰
        resampled = self._resample_df(source_df, pandas_rule)
        if resampled.empty:
            logger.error(f"[RESAMPLE] {ticker}/{tf} ë¦¬ìƒ˜í”Œë§ ê²°ê³¼ ë¹„ì–´ìˆìŒ")
            return pd.DataFrame()

        # ì €ì¥
        self.write_intraday(ticker, tf, resampled)
        logger.info(f"[RESAMPLE] {ticker} {tf} ì €ì¥ ({len(resampled)} bars)")

        return resampled

    def _resample_df(self, df: pd.DataFrame, rule: str) -> pd.DataFrame:
        """
        DataFrameì„ pandas resampleë¡œ OHLCV ì§‘ê³„

        ELI5: 1ë¶„ë´‰ 5ê°œë¥¼ í•˜ë‚˜ì˜ 5ë¶„ë´‰ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
              - Open: ì²« ë²ˆì§¸ ë´‰ì˜ ì‹œê°€
              - High: ê°€ì¥ ë†’ì€ ê³ ê°€
              - Low: ê°€ì¥ ë‚®ì€ ì €ê°€
              - Close: ë§ˆì§€ë§‰ ë´‰ì˜ ì¢…ê°€
              - Volume: ì „ì²´ ê±°ë˜ëŸ‰ í•©ê³„

        Args:
            df: ì†ŒìŠ¤ DataFrame (timestamp, open, high, low, close, volume í•„ìˆ˜)
            rule: pandas resample ê·œì¹™ (ì˜ˆ: "5min", "4h", "W-FRI")

        Returns:
            pd.DataFrame: ë¦¬ìƒ˜í”Œë§ëœ OHLCV ë°ì´í„°
        """
        if df.empty:
            return pd.DataFrame()

        # timestamp ì»¬ëŸ¼ì„ DatetimeIndexë¡œ ë³€í™˜
        # ELI5: timestampë¥¼ ë‚ ì§œ/ì‹œê°„ í˜•íƒœë¡œ ë°”ê¿”ì„œ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™” ê°€ëŠ¥í•˜ê²Œ
        df = df.copy()
        df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
        df = df.set_index("datetime")

        # OHLCV ë¦¬ìƒ˜í”Œë§ (ELI5: ì‹œê°„ ë²”ìœ„ë³„ë¡œ ë°ì´í„° ì§‘ê³„)
        resampled = df.resample(rule, closed="left", label="left").agg({
            "open": "first",
            "high": "max",
            "low": "min",
            "close": "last",
            "volume": "sum",
        }).dropna(subset=["open"])  # NaN í–‰ ì œê±°

        # timestamp ì»¬ëŸ¼ ë³µì›
        resampled = resampled.reset_index()
        resampled["timestamp"] = resampled["datetime"].astype("int64") // 10**6
        resampled = resampled.drop(columns=["datetime"])

        return resampled[["timestamp", "open", "high", "low", "close", "volume"]]

    def resample_all_tickers(
        self,
        target_tf: str,
        callback: Callable[[str, int, int], None] | None = None,
        max_history: timedelta = timedelta(weeks=2),
    ) -> int:
        """
        ëª¨ë“  í‹°ì»¤ì— ëŒ€í•´ target_tf ë¦¬ìƒ˜í”Œë§ ìˆ˜í–‰

        ELI5: ì €ì¥ëœ ëª¨ë“  1ë¶„ë´‰ íŒŒì¼ë“¤ì„ 5ë¶„ë´‰ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
              ì§„í–‰ ìƒí™©ì„ callbackìœ¼ë¡œ ì•Œë ¤ì¤ë‹ˆë‹¤.

        Args:
            target_tf: íƒ€ê²Ÿ íƒ€ì„í”„ë ˆì„ (ì˜ˆ: "5m", "15m")
            callback: ì§„í–‰ìƒí™© ì½œë°± (ticker, current, total) - GUI ì—°ë™ìš©
            max_history: ìµœëŒ€ ì´ë ¥ ê¸°ê°„ (ê¸°ë³¸ 2ì£¼)

        Returns:
            int: ì„±ê³µì ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ëœ í‹°ì»¤ ìˆ˜
        """
        rule = RESAMPLE_RULES.get(target_tf)
        if not rule:
            logger.error(f"[RESAMPLE-ALL] {target_tf}ì— ëŒ€í•œ ê·œì¹™ ì—†ìŒ")
            return 0

        source_tf = rule[0]
        tickers = self.get_intraday_tickers(source_tf)
        total = len(tickers)

        if total == 0:
            logger.warning(f"[RESAMPLE-ALL] {source_tf} í‹°ì»¤ ì—†ìŒ")
            return 0

        logger.info(f"[RESAMPLE-ALL] {target_tf} ì¼ê´„ ë¦¬ìƒ˜í”Œ ì‹œì‘ ({total} tickers)")

        success_count = 0
        for i, ticker in enumerate(tickers, 1):
            try:
                # ì½œë°± í˜¸ì¶œ (GUI ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ìš©)
                if callback:
                    callback(ticker, i, total)

                # ë¦¬ìƒ˜í”Œë§ ì‹œë„
                result = self.get_intraday_bars(ticker, target_tf, auto_fill=True)
                if not result.empty:
                    success_count += 1

            except Exception as e:
                logger.error(f"[RESAMPLE-ALL] {ticker} ì‹¤íŒ¨: {e}")

        logger.info(f"[RESAMPLE-ALL] ì™„ë£Œ: {success_count}/{total} ì„±ê³µ")
        return success_count

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ìœ í‹¸ë¦¬í‹°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_available_tickers(self) -> list[str]:
        """
        ì €ì¥ëœ ì¼ë´‰ ë°ì´í„°ì˜ í‹°ì»¤ ëª©ë¡ ë°˜í™˜

        Returns:
            list[str]: í‹°ì»¤ ëª©ë¡
        """
        if not self.daily_path.exists():
            return []

        df = pq.read_table(self.daily_path, columns=["ticker"]).to_pandas()
        return df["ticker"].unique().tolist()

    def get_intraday_tickers(self, timeframe: str = "1m") -> list[str]:
        """
        ì €ì¥ëœ intraday ë°ì´í„°ì˜ í‹°ì»¤ ëª©ë¡ ë°˜í™˜

        [11-003] íƒ€ì„í”„ë ˆì„ë³„ í´ë” êµ¬ì¡° ì§€ì›
        ì‹ ê·œ: {tf}/AAPL.parquet
        ë ˆê±°ì‹œ: intraday/AAPL_{tf}.parquet (fallback)

        Args:
            timeframe: íƒ€ì„í”„ë ˆì„ (ê¸°ë³¸ê°’: "1m")

        Returns:
            list[str]: í‹°ì»¤ ëª©ë¡
        """
        tickers = set()

        # [11-003] ìƒˆ êµ¬ì¡°ì—ì„œ ì¡°íšŒ: {tf}/*.parquet
        tf_dir = self._tf_dirs.get(timeframe, self.base_dir / timeframe)
        if tf_dir.exists():
            for f in tf_dir.glob("*.parquet"):
                # AAPL.parquet â†’ AAPL
                tickers.add(f.stem)

        # [11-003] ë ˆê±°ì‹œ êµ¬ì¡°ì—ì„œë„ ì¡°íšŒ (fallback): intraday/*_{tf}.parquet
        if self._legacy_intraday_dir.exists():
            pattern = f"*_{timeframe}.parquet"
            for f in self._legacy_intraday_dir.glob(pattern):
                # AAPL_1m.parquet â†’ AAPL
                ticker = f.stem.replace(f"_{timeframe}", "")
                tickers.add(ticker)

        return sorted(tickers)

    def get_stats(self) -> dict:
        """
        ì €ì¥ì†Œ í†µê³„ ë°˜í™˜

        [11-003] íƒ€ì„í”„ë ˆì„ë³„ í´ë” êµ¬ì¡° í†µê³„ ì§€ì›

        Returns:
            dict: í†µê³„ ì •ë³´
                - daily_rows: ì¼ë´‰ ë ˆì½”ë“œ ìˆ˜
                - daily_tickers: ì¼ë´‰ í‹°ì»¤ ìˆ˜
                - daily_file_size_mb: ì¼ë´‰ íŒŒì¼ í¬ê¸° (MB)
                - intraday_files: ë¶„ë´‰ íŒŒì¼ ìˆ˜ (ì „ì²´)
                - intraday_by_tf: íƒ€ì„í”„ë ˆì„ë³„ íŒŒì¼ ìˆ˜
        """
        stats = {
            "daily_rows": 0,
            "daily_tickers": 0,
            "daily_file_size_mb": 0.0,
            "intraday_files": 0,
            "intraday_by_tf": {},
        }

        if self.daily_path.exists():
            df = pq.read_table(self.daily_path).to_pandas()
            stats["daily_rows"] = len(df)
            stats["daily_tickers"] = df["ticker"].nunique()
            stats["daily_file_size_mb"] = self.daily_path.stat().st_size / (1024 * 1024)

        # [11-003] TFë³„ í´ë”ì—ì„œ íŒŒì¼ ìˆ˜ ì§‘ê³„
        total_intraday = 0
        for tf, tf_dir in self._tf_dirs.items():
            count = len(list(tf_dir.glob("*.parquet")))
            stats["intraday_by_tf"][tf] = count
            total_intraday += count

        # ë ˆê±°ì‹œ í´ë”ë„ í¬í•¨
        if self._legacy_intraday_dir.exists():
            legacy_count = len(list(self._legacy_intraday_dir.glob("*.parquet")))
            stats["intraday_by_tf"]["legacy_intraday"] = legacy_count
            total_intraday += legacy_count

        stats["intraday_files"] = total_intraday

        return stats

    def delete_ticker_intraday(self, ticker: str) -> bool:
        """
        íŠ¹ì • í‹°ì»¤ì˜ ëª¨ë“  Intraday íŒŒì¼ ì‚­ì œ

        [11-003] ìƒˆ êµ¬ì¡°ì™€ ë ˆê±°ì‹œ êµ¬ì¡° ëª¨ë‘ ì‚­ì œ

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼

        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        deleted = False

        # [11-003] ìƒˆ êµ¬ì¡°ì—ì„œ ì‚­ì œ: {tf}/{ticker}.parquet
        for tf in self.SUPPORTED_TIMEFRAMES:
            path = self._get_intraday_path(ticker, tf)
            if path.exists():
                path.unlink()
                deleted = True
                logger.info(f"ğŸ—‘ï¸ Deleted: {path}")

        # [11-003] ë ˆê±°ì‹œ êµ¬ì¡°ì—ì„œë„ ì‚­ì œ: intraday/{ticker}_{tf}.parquet
        if self._legacy_intraday_dir.exists():
            for tf in ["1m", "3m", "5m", "15m", "1h", "4h"]:
                legacy_path = self._legacy_intraday_dir / f"{ticker}_{tf}.parquet"
                if legacy_path.exists():
                    legacy_path.unlink()
                    deleted = True
                    logger.info(f"ğŸ—‘ï¸ Deleted (legacy): {legacy_path}")

        return deleted
