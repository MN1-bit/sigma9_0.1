# ============================================================================
# Data Repository - í†µí•© ë°ì´í„° ì ‘ê·¼ ë ˆì´ì–´
# ============================================================================
# ğŸ“Œ ì´ íŒŒì¼ì˜ ì—­í• :
#   - ëª¨ë“  ì‹œì¥ ë°ì´í„° ì ‘ê·¼ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©
#   - Parquetì„ Primary Storageë¡œ ì‚¬ìš©
#   - On-Demand Gap Fill ì§€ì› (ëˆ„ë½ ë°ì´í„° ìë™ API í˜¸ì¶œ)
#   - ë³´ì¡°ì§€í‘œ ìºì‹± + ìŠ¤ì½”ì–´ FlushPolicy ì ìš©
#
# ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
#   >>> repo = DataRepository(parquet_manager, massive_client)
#   >>> df = await repo.get_daily_bars("AAPL", days=60)
#   >>> repo.update_score("AAPL", "v3", {"score": 85, ...})
#
# ğŸ“Œ [11-002] DataRepository ë¦¬íŒ©í„°ë§
# ============================================================================

from pathlib import Path
from typing import Any, Optional
import time
import pandas as pd
from loguru import logger

from backend.data.parquet_manager import ParquetManager
from backend.data.flush_policy import FlushPolicy, IntervalFlush


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DataRepository í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DataRepository:
    """
    í†µí•© ë°ì´í„° ì ‘ê·¼ ë ˆì´ì–´

    ëª¨ë“  ì‹œì¥ ë°ì´í„° ì ‘ê·¼ì€ ì´ í´ë˜ìŠ¤ë¥¼ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
    Parquetì„ Primary Storageë¡œ ì‚¬ìš©í•˜ë©°, On-Demand Gap Fillì„ ì§€ì›í•©ë‹ˆë‹¤.

    ELI5: ë°ì´í„°ê°€ í•„ìš”í•˜ë©´ ì´ í´ë˜ìŠ¤í•œí…Œ ë¬¼ì–´ë³´ì„¸ìš”.
          ë¡œì»¬ì— ì—†ìœ¼ë©´ ì•Œì•„ì„œ API í˜¸ì¶œí•´ì„œ ê°€ì ¸ì™€ ì¤ë‹ˆë‹¤.

    Attributes:
        _pm: ParquetManager ì¸ìŠ¤í„´ìŠ¤ (Low-Level I/O)
        _client: MassiveClient ì¸ìŠ¤í„´ìŠ¤ (API í˜¸ì¶œìš©, None ê°€ëŠ¥)
        _flush_policy: ìŠ¤ì½”ì–´ Flush ì •ì±…
        _score_cache: ë©”ëª¨ë¦¬ ìŠ¤ì½”ì–´ ìºì‹œ
        _indicator_cache: ë³´ì¡°ì§€í‘œ ë©”ëª¨ë¦¬ ìºì‹œ

    Example:
        >>> pm = ParquetManager("data/parquet")
        >>> repo = DataRepository(pm, massive_client=client)
        >>> df = await repo.get_daily_bars("AAPL", days=60)
    """

    def __init__(
        self,
        parquet_manager: ParquetManager,
        massive_client: Optional[Any] = None,
        flush_policy: Optional[FlushPolicy] = None,
    ):
        """
        DataRepository ì´ˆê¸°í™”

        Args:
            parquet_manager: Parquet I/O ë‹´ë‹¹ (í•„ìˆ˜)
            massive_client: Massive API í´ë¼ì´ì–¸íŠ¸ (Gap Fillìš©, ì„ íƒ)
            flush_policy: ìŠ¤ì½”ì–´ Flush ì •ì±… (ê¸°ë³¸: IntervalFlush(30ì´ˆ))
        """
        # í•µì‹¬ ì˜ì¡´ì„±
        self._pm = parquet_manager
        self._client = massive_client

        # FlushPolicy (ELI5: ìŠ¤ì½”ì–´ë¥¼ ì–¸ì œ íŒŒì¼ì— ì €ì¥í• ì§€ ê²°ì •)
        self._flush_policy = flush_policy or IntervalFlush(interval_seconds=30.0)

        # ìŠ¤ì½”ì–´ ìºì‹œ (ë©”ëª¨ë¦¬) - {ticker: score_data}
        self._score_cache: dict[str, dict[str, Any]] = {}
        self._last_flush = time.time()
        self._update_count = 0

        # ë³´ì¡°ì§€í‘œ ìºì‹œ ê²½ë¡œ
        self._indicator_dir = Path(self._pm.base_dir) / "indicators"
        self._indicator_dir.mkdir(parents=True, exist_ok=True)

        # ìŠ¤ì½”ì–´ ì €ì¥ ê²½ë¡œ
        self._scores_dir = Path(self._pm.base_dir) / "scores"
        self._scores_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"ğŸ“¦ DataRepository initialized (FlushPolicy: {type(self._flush_policy).__name__})")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Daily/Intraday Data (auto_fill=True ê¸°ë³¸ê°’)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def get_daily_bars(
        self,
        ticker: str,
        days: int = 60,
        *,
        auto_fill: bool = True,
    ) -> pd.DataFrame:
        """
        ì¼ë´‰ ë°ì´í„° ì¡°íšŒ (ëˆ„ë½ ì‹œ API ìë™ í˜¸ì¶œ)

        ELI5: "AAPL 60ì¼ì¹˜ ì¼ë´‰ ì¤˜" â†’ ì—†ìœ¼ë©´ APIì—ì„œ ê°€ì ¸ì™€ì„œ ì €ì¥ í›„ ë°˜í™˜

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼ (ì˜ˆ: "AAPL")
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 60ì¼)
            auto_fill: Trueë©´ ëˆ„ë½ ë°ì´í„° API í˜¸ì¶œ í›„ ì €ì¥ (ê¸°ë³¸ê°’: True)

        Returns:
            pd.DataFrame: ì¼ë´‰ ë°ì´í„° (ë¹ˆ ê²½ìš° ë¹ˆ DataFrame)
        """
        # 1. ë¡œì»¬ Parquetì—ì„œ ë¨¼ì € ì¡°íšŒ
        df = self._pm.read_daily(ticker, days)

        # 2. auto_fill=Trueì´ê³  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ Gap Fill
        if auto_fill and self._has_daily_gaps(df, ticker, days):
            await self._fill_daily_gaps(ticker, days)
            # ë‹¤ì‹œ ì¡°íšŒ
            df = self._pm.read_daily(ticker, days)

        return df

    async def get_intraday_bars(
        self,
        ticker: str,
        timeframe: str,
        days: int = 2,
        *,
        auto_fill: bool = True,
    ) -> pd.DataFrame:
        """
        ë¶„ë´‰/ì‹œë´‰ ë°ì´í„° ì¡°íšŒ (ëˆ„ë½ ì‹œ API ìë™ í˜¸ì¶œ)

        ELI5: "AAPL 1ë¶„ë´‰ 2ì¼ì¹˜ ì¤˜" â†’ ì—†ìœ¼ë©´ APIì—ì„œ ê°€ì ¸ì˜´

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„ ("1m", "5m", "15m", "1h")
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 2ì¼)
            auto_fill: Trueë©´ ëˆ„ë½ ë°ì´í„° API í˜¸ì¶œ í›„ ì €ì¥

        Returns:
            pd.DataFrame: Intraday ë°ì´í„°
        """
        # 1. ë¡œì»¬ì—ì„œ ë¨¼ì € ì¡°íšŒ
        df = self._pm.read_intraday(ticker, timeframe, days)

        # 2. auto_fill=Trueì´ê³  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ Gap Fill
        if auto_fill and self._has_intraday_gaps(df, ticker, timeframe, days):
            await self._fill_intraday_gaps(ticker, timeframe, days)
            df = self._pm.read_intraday(ticker, timeframe, days)

        return df

    def get_all_tickers(self) -> list[str]:
        """
        ì €ì¥ëœ ì¼ë´‰ ë°ì´í„°ì˜ í‹°ì»¤ ëª©ë¡

        Returns:
            list[str]: ì‚¬ìš© ê°€ëŠ¥í•œ í‹°ì»¤ ëª©ë¡
        """
        return self._pm.get_available_tickers()

    def get_daily_bars_bulk(
        self,
        tickers: list[str] | None = None,
        days: int = 20,
    ) -> dict[str, list[dict]]:
        """
        [12-002] ì—¬ëŸ¬ í‹°ì»¤ì˜ ì¼ë´‰ ë°ì´í„°ë¥¼ ë²Œí¬ë¡œ ì¡°íšŒ

        ELI5: Parquet íŒŒì¼ 1íšŒ ì½ê¸°ë¡œ ëª¨ë“  í‹°ì»¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
              í‹°ì»¤ 10,000ê°œë¥¼ ì¡°íšŒí•´ë„ íŒŒì¼ I/OëŠ” 1ë²ˆë§Œ ë°œìƒí•©ë‹ˆë‹¤.

        ì£¼ì˜: auto_fillì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¡œì»¬ ë°ì´í„°ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
              API í˜¸ì¶œì´ í•„ìš”í•˜ë©´ get_daily_bars()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

        Args:
            tickers: ì¡°íšŒí•  í‹°ì»¤ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 20)

        Returns:
            dict[str, list[dict]]: í‹°ì»¤ â†’ ì¼ë´‰ ë°ì´í„° (ë‚ ì§œìˆœ ì •ë ¬)
                ì˜ˆ: {"AAPL": [{"date": "2024-01-01", ...}, ...], ...}
        """
        return self._pm.read_daily_bulk(tickers=tickers, days=days)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Gap Detection & Fill (ëˆ„ë½ ê°ì§€ ë° ë³´ì¶©)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _has_daily_gaps(self, df: pd.DataFrame, ticker: str, days: int) -> bool:
        """
        ì¼ë´‰ ë°ì´í„° ëˆ„ë½ ì—¬ë¶€ íŒë‹¨

        ELI5: 60ì¼ì¹˜ ìš”ì²­í–ˆëŠ”ë° 30ì¼ì¹˜ë§Œ ìˆìœ¼ë©´ "ëˆ„ë½"

        Args:
            df: í˜„ì¬ ì¡°íšŒëœ ë°ì´í„°
            ticker: í‹°ì»¤ (ë¡œê¹…ìš©)
            days: ìš”ì²­í•œ ì¼ìˆ˜

        Returns:
            bool: Trueë©´ Gap Fill í•„ìš”
        """
        if df.empty:
            # ë°ì´í„° ì—†ìŒ = Gap
            logger.debug(f"ğŸ“­ No daily data for {ticker}, gap fill needed")
            return True

        # ì‹¤ì œ ê±°ë˜ì¼ ìˆ˜ ê³„ì‚° (ì£¼ë§ ì œì™¸í•˜ë©´ ì•½ 70%)
        expected_trading_days = int(days * 0.7)

        if len(df) < expected_trading_days:
            logger.debug(
                f"ğŸ“­ Insufficient daily data for {ticker}: "
                f"{len(df)}/{expected_trading_days} expected"
            )
            return True

        return False

    def _has_intraday_gaps(
        self, df: pd.DataFrame, ticker: str, timeframe: str, days: int
    ) -> bool:
        """
        Intraday ë°ì´í„° ëˆ„ë½ ì—¬ë¶€ íŒë‹¨

        Args:
            df: í˜„ì¬ ì¡°íšŒëœ ë°ì´í„°
            ticker: í‹°ì»¤
            timeframe: íƒ€ì„í”„ë ˆì„
            days: ìš”ì²­í•œ ì¼ìˆ˜

        Returns:
            bool: Trueë©´ Gap Fill í•„ìš”
        """
        if df.empty:
            logger.debug(f"ğŸ“­ No intraday data for {ticker}_{timeframe}, gap fill needed")
            return True

        # 1ë¶„ë´‰ ê¸°ì¤€ í•˜ë£¨ ì•½ 390ë¶„ (6.5ì‹œê°„ * 60ë¶„)
        # ì‹œë´‰ ê¸°ì¤€ í•˜ë£¨ ì•½ 7ì‹œê°„
        bars_per_day = {"1m": 390, "5m": 78, "15m": 26, "1h": 7}.get(timeframe, 390)
        expected_bars = bars_per_day * days * 0.5  # ë³´ìˆ˜ì ìœ¼ë¡œ 50%

        if len(df) < expected_bars:
            logger.debug(
                f"ğŸ“­ Insufficient intraday data for {ticker}_{timeframe}: "
                f"{len(df)}/{int(expected_bars)} expected"
            )
            return True

        return False

    async def _fill_daily_gaps(self, ticker: str, days: int) -> None:
        """
        ì¼ë´‰ Gap Fill (Massive API í˜¸ì¶œ)

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            days: ì¡°íšŒí•  ì¼ìˆ˜
        """
        if not self._client:
            logger.warning(f"âš ï¸ Cannot fill daily gaps for {ticker}: no API client")
            return

        try:
            logger.info(f"ğŸ”„ Filling daily gaps for {ticker} ({days} days)")

            # Massive API í˜¸ì¶œ (daily bars)
            bars = await self._client.get_bars(ticker, interval="1d", limit=days)

            if not bars:
                logger.warning(f"âš ï¸ No daily bars returned for {ticker}")
                return

            # DataFrame ë³€í™˜ ë° ì €ì¥
            df = self._bars_to_daily_df(ticker, bars)
            if not df.empty:
                self._pm.append_daily(df)
                logger.info(f"âœ… Daily gap filled for {ticker}: {len(df)} bars")

        except Exception as e:
            logger.error(f"âŒ Failed to fill daily gaps for {ticker}: {e}")

    async def _fill_intraday_gaps(
        self, ticker: str, timeframe: str, days: int
    ) -> None:
        """
        Intraday Gap Fill (Massive API í˜¸ì¶œ)

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            days: ì¡°íšŒí•  ì¼ìˆ˜
        """
        if not self._client:
            logger.warning(f"âš ï¸ Cannot fill intraday gaps for {ticker}: no API client")
            return

        try:
            logger.info(f"ğŸ”„ Filling intraday gaps for {ticker}_{timeframe} ({days} days)")

            # Massive API í˜¸ì¶œ
            # timeframe ë³€í™˜: "1m" -> "1min", "1h" -> "1hour"
            api_interval = self._timeframe_to_api_interval(timeframe)
            bars = await self._client.get_bars(ticker, interval=api_interval, limit=days * 400)

            if not bars:
                logger.warning(f"âš ï¸ No intraday bars returned for {ticker}")
                return

            # DataFrame ë³€í™˜ ë° ì €ì¥
            df = self._bars_to_intraday_df(bars)
            if not df.empty:
                self._pm.append_intraday(ticker, timeframe, df)
                logger.info(f"âœ… Intraday gap filled for {ticker}_{timeframe}: {len(df)} bars")

        except Exception as e:
            logger.error(f"âŒ Failed to fill intraday gaps for {ticker}: {e}")

    def _timeframe_to_api_interval(self, timeframe: str) -> str:
        """
        ParquetManager timeframe â†’ Massive API interval ë³€í™˜

        Args:
            timeframe: "1m", "5m", "15m", "1h"

        Returns:
            str: API interval ("1min", "5min", "15min", "1hour")
        """
        mapping = {
            "1m": "1min",
            "5m": "5min",
            "15m": "15min",
            "1h": "1hour",
        }
        return mapping.get(timeframe, "1min")

    def _bars_to_daily_df(self, ticker: str, bars: list[dict]) -> pd.DataFrame:
        """
        API ì‘ë‹µì„ ì¼ë´‰ DataFrameìœ¼ë¡œ ë³€í™˜

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            bars: API ì‘ë‹µ ë°” ë¦¬ìŠ¤íŠ¸

        Returns:
            pd.DataFrame: ì¼ë´‰ ë°ì´í„°
        """
        if not bars:
            return pd.DataFrame()

        records = []
        for bar in bars:
            records.append({
                "ticker": ticker,
                "date": bar.get("date") or bar.get("t", ""),
                "open": bar.get("open") or bar.get("o", 0),
                "high": bar.get("high") or bar.get("h", 0),
                "low": bar.get("low") or bar.get("l", 0),
                "close": bar.get("close") or bar.get("c", 0),
                "volume": bar.get("volume") or bar.get("v", 0),
            })

        return pd.DataFrame(records)

    def _bars_to_intraday_df(self, bars: list[dict]) -> pd.DataFrame:
        """
        API ì‘ë‹µì„ Intraday DataFrameìœ¼ë¡œ ë³€í™˜

        Args:
            bars: API ì‘ë‹µ ë°” ë¦¬ìŠ¤íŠ¸

        Returns:
            pd.DataFrame: Intraday ë°ì´í„°
        """
        if not bars:
            return pd.DataFrame()

        records = []
        for bar in bars:
            records.append({
                "timestamp": bar.get("timestamp") or bar.get("t", 0),
                "open": bar.get("open") or bar.get("o", 0),
                "high": bar.get("high") or bar.get("h", 0),
                "low": bar.get("low") or bar.get("l", 0),
                "close": bar.get("close") or bar.get("c", 0),
                "volume": bar.get("volume") or bar.get("v", 0),
            })

        return pd.DataFrame(records)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Indicators (On-Demand ìƒì‚° + ì €ì¥)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_indicator(
        self,
        ticker: str,
        indicator: str,
        days: int = 60,
    ) -> Optional[pd.Series]:
        """
        ë³´ì¡°ì§€í‘œ ì¡°íšŒ (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ ê³„ì‚° í›„ ì €ì¥)

        ELI5: "SMA 20ì¼ ì¤˜" â†’ ì´ë¯¸ ê³„ì‚°í–ˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜,
              ì—†ìœ¼ë©´ ê³„ì‚°í•´ì„œ ì €ì¥ í›„ ë°˜í™˜

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            indicator: ì§€í‘œ ì´ë¦„ (ì˜ˆ: "sma_20", "rsi_14")
            days: ê³„ì‚°ì— ì‚¬ìš©í•  ì¼ìˆ˜

        Returns:
            pd.Series: ê³„ì‚°ëœ ì§€í‘œ (ì—†ìœ¼ë©´ None)
        """
        # 1. ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached = self._load_indicator_cache(ticker, indicator)
        if cached is not None:
            return cached

        # 2. ê³„ì‚°
        result = self._calculate_indicator(ticker, indicator, days)

        # 3. ì €ì¥ (On-Demand ìƒì‚° ì‹œ í•­ìƒ ì €ì¥)
        if result is not None:
            self._save_indicator_cache(ticker, indicator, result)

        return result

    def _load_indicator_cache(self, ticker: str, indicator: str) -> Optional[pd.Series]:
        """
        ë³´ì¡°ì§€í‘œ ìºì‹œ ë¡œë“œ

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            indicator: ì§€í‘œ ì´ë¦„

        Returns:
            pd.Series: ìºì‹œëœ ì§€í‘œ (ì—†ìœ¼ë©´ None)
        """
        path = self._indicator_dir / f"{indicator}_{ticker}.parquet"
        if not path.exists():
            return None

        try:
            import pyarrow.parquet as pq
            df = pq.read_table(path).to_pandas()
            if "value" in df.columns:
                return df["value"]
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load indicator cache: {e}")
            return None

    def _save_indicator_cache(self, ticker: str, indicator: str, data: pd.Series) -> None:
        """
        ë³´ì¡°ì§€í‘œ ìºì‹œ ì €ì¥

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            indicator: ì§€í‘œ ì´ë¦„
            data: ê³„ì‚°ëœ ì§€í‘œ ì‹œë¦¬ì¦ˆ
        """
        path = self._indicator_dir / f"{indicator}_{ticker}.parquet"
        try:
            import pyarrow as pa
            import pyarrow.parquet as pq

            df = pd.DataFrame({"value": data})
            pq.write_table(pa.Table.from_pandas(df), path, compression="snappy")
            logger.debug(f"ğŸ’¾ Indicator cached: {indicator}_{ticker}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save indicator cache: {e}")

    def _calculate_indicator(
        self, ticker: str, indicator: str, days: int
    ) -> Optional[pd.Series]:
        """
        ë³´ì¡°ì§€í‘œ ê³„ì‚°

        ì§€ì› ì§€í‘œ:
            - sma_{period}: ë‹¨ìˆœ ì´ë™í‰ê· 
            - ema_{period}: ì§€ìˆ˜ ì´ë™í‰ê· 
            - rsi_{period}: RSI

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            indicator: ì§€í‘œ ì´ë¦„ (ì˜ˆ: "sma_20")
            days: ê³„ì‚°ì— ì‚¬ìš©í•  ì¼ìˆ˜

        Returns:
            pd.Series: ê³„ì‚°ëœ ì§€í‘œ (ì‹¤íŒ¨ ì‹œ None)
        """
        # ì¼ë´‰ ë°ì´í„° ì¡°íšŒ (ë™ê¸° ë²„ì „, cache only)
        df = self._pm.read_daily(ticker, days)
        if df.empty or "close" not in df.columns:
            return None

        close = df["close"]

        # ì§€í‘œ íŒŒì‹± (ì˜ˆ: "sma_20" â†’ type="sma", period=20)
        parts = indicator.split("_")
        if len(parts) != 2:
            logger.warning(f"âš ï¸ Unknown indicator format: {indicator}")
            return None

        ind_type, period_str = parts
        try:
            period = int(period_str)
        except ValueError:
            logger.warning(f"âš ï¸ Invalid indicator period: {indicator}")
            return None

        # ì§€í‘œ ê³„ì‚°
        if ind_type == "sma":
            return close.rolling(window=period).mean()
        elif ind_type == "ema":
            return close.ewm(span=period, adjust=False).mean()
        elif ind_type == "rsi":
            delta = close.diff()
            gain = delta.where(delta > 0, 0).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss.replace(0, float("nan"))
            return 100 - (100 / (1 + rs))
        else:
            logger.warning(f"âš ï¸ Unsupported indicator type: {ind_type}")
            return None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Scores (ë©”ëª¨ë¦¬ ìºì‹œ + ì„¤ì • ê¸°ë°˜ Flush)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def update_score(
        self,
        ticker: str,
        version: str,
        score_data: dict[str, Any],
    ) -> None:
        """
        ìŠ¤ì½”ì–´ ì—…ë°ì´íŠ¸ (ê°±ì‹  ì£¼ê¸°ì— ë”°ë¼ í˜¸ì¶œ)

        ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥í•˜ê³ , FlushPolicyì— ë”°ë¼ Parquet ì €ì¥

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼
            version: ìŠ¤ì½”ì–´ ë²„ì „ (ì˜ˆ: "v3")
            score_data: ìŠ¤ì½”ì–´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self._score_cache[ticker] = {
            "ticker": ticker,
            **score_data,
        }
        self._update_count += 1

        # FlushPolicyì— ë”°ë¼ ì €ì¥ ì—¬ë¶€ ê²°ì •
        if self._flush_policy.should_flush(self._last_flush, self._update_count):
            self._flush_scores(version)

    def get_score(self, ticker: str) -> dict[str, Any]:
        """
        ìŠ¤ì½”ì–´ ì¡°íšŒ (ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„ )

        Args:
            ticker: ì¢…ëª© ì‹¬ë³¼

        Returns:
            dict: ìŠ¤ì½”ì–´ ë°ì´í„° (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        """
        return self._score_cache.get(ticker, {})

    def get_all_scores(self) -> dict[str, dict[str, Any]]:
        """
        ì „ì²´ ìŠ¤ì½”ì–´ ìºì‹œ ë°˜í™˜

        Returns:
            dict: {ticker: score_data} í˜•íƒœ
        """
        return self._score_cache.copy()

    def _flush_scores(self, version: str = "v3") -> None:
        """
        ìŠ¤ì½”ì–´ Parquet ì €ì¥ (ë‚´ë¶€ í˜¸ì¶œ)

        Args:
            version: ìŠ¤ì½”ì–´ ë²„ì „
        """
        if not self._score_cache:
            return

        try:
            import pyarrow as pa
            import pyarrow.parquet as pq

            df = pd.DataFrame(list(self._score_cache.values()))
            path = self._scores_dir / f"current_{version}.parquet"
            pq.write_table(pa.Table.from_pandas(df), path, compression="snappy")

            # ìƒíƒœ ë¦¬ì…‹
            self._last_flush = time.time()
            self._update_count = 0

            logger.debug(f"ğŸ’¾ Scores flushed: {len(df)} tickers â†’ {path}")
        except Exception as e:
            logger.error(f"âŒ Failed to flush scores: {e}")

    def force_flush(self, version: str = "v3") -> None:
        """
        ê°•ì œ Flush (ì¥ ë§ˆê°, ì„œë²„ ì¢…ë£Œ ì‹œ í˜¸ì¶œ)

        Args:
            version: ìŠ¤ì½”ì–´ ë²„ì „
        """
        logger.info("âš¡ Force flushing scores...")
        self._flush_scores(version)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Utilities
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_stats(self) -> dict[str, Any]:
        """
        DataRepository í†µê³„ ë°˜í™˜

        Returns:
            dict: í†µê³„ ì •ë³´
        """
        pm_stats = self._pm.get_stats()
        return {
            **pm_stats,
            "score_cache_size": len(self._score_cache),
            "flush_policy": type(self._flush_policy).__name__,
            "update_count": self._update_count,
        }
