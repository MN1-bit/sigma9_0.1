# ============================================================================
# Parquet ë°ì´í„° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ (11-004)
# ============================================================================
# ğŸ“Œ ì´ íŒŒì¼ì˜ ì—­í• :
#   - ì¤‘ë³µ ë ˆì½”ë“œ ìë™ ì œê±°
#   - NULL ê°’ ë³´ê°„/ì‚­ì œ
#   - Dry-run ëª¨ë“œ ì§€ì›
#   - ë³µêµ¬ ì „ ìë™ ë°±ì—… (ë³€ê²½ íŒŒì¼ë§Œ)
#
# ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
#   >>> python -m backend.scripts.repair_parquet_data --dry-run
#   >>> python -m backend.scripts.repair_parquet_data --apply
# ============================================================================

"""
Parquet ë°ì´í„° ë³µêµ¬ CLI (11-004)

ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸.

ê¸°ëŠ¥:
1. ì¤‘ë³µ ë ˆì½”ë“œ ì œê±° (ticker+date ê¸°ì¤€)
2. NULL ê°’ ì²˜ë¦¬ (forward fill / linear interpolation)
3. Dry-run ëª¨ë“œ (ì‹¤ì œ ìˆ˜ì • ì—†ì´ ì‹œë®¬ë ˆì´ì…˜)
4. ë³€ê²½ íŒŒì¼ë§Œ ë°±ì—…

ELI5: ë¬¸ì œ ìˆëŠ” ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê³ ì³ì£¼ëŠ” ì˜ì‚¬.
"""

import argparse
import json
import shutil
import sys
from datetime import datetime
from pathlib import Path

import pyarrow.parquet as pq
from loguru import logger


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DataRepairer í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DataRepairer:
    """
    Parquet ë°ì´í„° ë³µêµ¬ í´ë˜ìŠ¤

    ì¤‘ë³µ ì œê±°, NULL ì²˜ë¦¬ ë“± ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

    Attributes:
        base_dir: Parquet ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬
        backup_dir: ë°±ì—… ì €ì¥ ë””ë ‰í„°ë¦¬
        dry_run: Trueë©´ ì‹¤ì œ ìˆ˜ì • ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ

    Example:
        >>> repairer = DataRepairer(Path("data/parquet"), dry_run=True)
        >>> report = repairer.repair_all()
        >>> print(report)
    """

    def __init__(
        self,
        base_dir: Path,
        backup_dir: Path = None,
        dry_run: bool = True,
    ):
        """
        DataRepairer ì´ˆê¸°í™”

        Args:
            base_dir: Parquet ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬
            backup_dir: ë°±ì—… ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: data/backup)
            dry_run: Trueë©´ ì‹¤ì œ ìˆ˜ì • ì—†ì´ ì‹œë®¬ë ˆì´ì…˜
        """
        self.base_dir = Path(base_dir)
        self.backup_dir = Path(backup_dir) if backup_dir else Path("data/backup")
        self.dry_run = dry_run

        # ë³µêµ¬ ë¦¬í¬íŠ¸
        self.report: dict = {
            "started_at": datetime.now().isoformat(),
            "dry_run": dry_run,
            "actions": [],
            "errors": [],
        }

        logger.info(f"ğŸ”§ DataRepairer ì´ˆê¸°í™”: base_dir={base_dir}, dry_run={dry_run}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ë°±ì—…
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def backup_file(self, file_path: Path) -> Path | None:
        """
        íŒŒì¼ ë°±ì—… ìƒì„±

        Args:
            file_path: ë°±ì—…í•  íŒŒì¼ ê²½ë¡œ

        Returns:
            Path: ë°±ì—… íŒŒì¼ ê²½ë¡œ (dry_runì´ë©´ None)
        """
        if self.dry_run:
            logger.info(f"  [DRY-RUN] ë°±ì—… ìƒëµ: {file_path.name}")
            return None

        # ë°±ì—… ë””ë ‰í„°ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_subdir = self.backup_dir / timestamp
        backup_subdir.mkdir(parents=True, exist_ok=True)

        # ìƒëŒ€ ê²½ë¡œ ìœ ì§€í•˜ì—¬ ë°±ì—…
        relative_path = file_path.relative_to(self.base_dir)
        backup_path = backup_subdir / relative_path
        backup_path.parent.mkdir(parents=True, exist_ok=True)

        shutil.copy2(file_path, backup_path)
        logger.info(f"  ğŸ’¾ ë°±ì—… ì™„ë£Œ: {backup_path}")

        return backup_path

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì¤‘ë³µ ì œê±°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def remove_duplicates_daily(self) -> int:
        """
        Daily ë°ì´í„°ì—ì„œ ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°

        (ticker, date) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°, ë§ˆì§€ë§‰ ë ˆì½”ë“œ ìœ ì§€.

        Returns:
            int: ì œê±°ëœ ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜
        """
        daily_path = self.base_dir / "daily" / "all_daily.parquet"

        if not daily_path.exists():
            logger.warning("âš ï¸ all_daily.parquet ì—†ìŒ")
            return 0

        try:
            df = pq.read_table(daily_path).to_pandas()
            original_count = len(df)

            # ì¤‘ë³µ í™•ì¸
            dups = df.duplicated(subset=["ticker", "date"], keep="last")
            dup_count = dups.sum()

            if dup_count == 0:
                logger.info("âœ… Daily ë°ì´í„°: ì¤‘ë³µ ì—†ìŒ")
                return 0

            logger.info(f"ğŸ” Daily ì¤‘ë³µ ë°œê²¬: {dup_count}ê±´")

            # ì¤‘ë³µ ì œê±°
            df_dedup = df[~dups].reset_index(drop=True)

            if not self.dry_run:
                # ë°±ì—… í›„ ì €ì¥
                self.backup_file(daily_path)
                df_dedup.to_parquet(daily_path, index=False)
                logger.info(f"âœ… Daily ì¤‘ë³µ ì œê±° ì™„ë£Œ: {dup_count}ê±´")
            else:
                logger.info(f"  [DRY-RUN] ì¤‘ë³µ ì œê±° ì˜ˆì •: {dup_count}ê±´")

            self.report["actions"].append(
                {
                    "type": "remove_duplicates",
                    "file": str(daily_path),
                    "removed": dup_count,
                    "original": original_count,
                    "final": len(df_dedup),
                }
            )

            return dup_count

        except Exception as e:
            logger.error(f"âŒ Daily ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
            self.report["errors"].append(
                {
                    "type": "remove_duplicates",
                    "file": str(daily_path),
                    "error": str(e),
                }
            )
            return 0

    def remove_duplicates_intraday(self) -> int:
        """
        Intraday ë°ì´í„°ì—ì„œ ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°

        timestamp ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°.

        Returns:
            int: ì´ ì œê±°ëœ ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜
        """
        total_removed = 0
        tf_folders = ["1m", "3m", "5m", "15m", "1h", "4h"]

        for tf in tf_folders:
            tf_dir = self.base_dir / tf
            if not tf_dir.exists():
                continue

            for f in tf_dir.glob("*.parquet"):
                try:
                    df = pq.read_table(f).to_pandas()

                    dups = df.duplicated(subset=["timestamp"], keep="last")
                    dup_count = dups.sum()

                    if dup_count == 0:
                        continue

                    df_dedup = df[~dups].reset_index(drop=True)

                    if not self.dry_run:
                        self.backup_file(f)
                        df_dedup.to_parquet(f, index=False)

                    total_removed += dup_count

                    self.report["actions"].append(
                        {
                            "type": "remove_duplicates",
                            "file": str(f),
                            "removed": dup_count,
                        }
                    )

                    logger.info(
                        f"  {'[DRY-RUN] ' if self.dry_run else ''}"
                        f"{tf}/{f.name}: ì¤‘ë³µ {dup_count}ê±´ ì œê±°"
                    )

                except Exception as e:
                    logger.error(f"âŒ {f}: ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")
                    self.report["errors"].append(
                        {
                            "type": "remove_duplicates",
                            "file": str(f),
                            "error": str(e),
                        }
                    )

        return total_removed

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NULL ì²˜ë¦¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def fill_nulls_daily(self, strategy: str = "ffill") -> int:
        """
        Daily ë°ì´í„°ì—ì„œ NULL ê°’ ì²˜ë¦¬

        OHLCV ì»¬ëŸ¼ì˜ NULLì„ ë³´ê°„í•©ë‹ˆë‹¤.

        Args:
            strategy: ë³´ê°„ ì „ëµ ('ffill', 'linear', 'drop')

        Returns:
            int: ì²˜ë¦¬ëœ NULL ì…€ ìˆ˜
        """
        daily_path = self.base_dir / "daily" / "all_daily.parquet"

        if not daily_path.exists():
            return 0

        try:
            df = pq.read_table(daily_path).to_pandas()

            # OHLCV ì»¬ëŸ¼ë§Œ ëŒ€ìƒ
            price_cols = ["open", "high", "low", "close", "volume"]
            null_counts = df[price_cols].isnull().sum()
            total_nulls = null_counts.sum()

            if total_nulls == 0:
                logger.info("âœ… Daily ë°ì´í„°: NULL ì—†ìŒ")
                return 0

            logger.info(f"ğŸ” Daily NULL ë°œê²¬: {total_nulls}ê±´")
            logger.debug(f"  ì»¬ëŸ¼ë³„: {null_counts.to_dict()}")

            # NULL ì²˜ë¦¬
            if strategy == "drop":
                # NULL ìˆëŠ” í–‰ ì‚­ì œ
                df_clean = df.dropna(subset=price_cols)
            elif strategy == "linear":
                # ì„ í˜• ë³´ê°„
                df_clean = df.copy()
                df_clean[price_cols] = df_clean[price_cols].interpolate(method="linear")
            else:
                # Forward fill (ê¸°ë³¸ê°’)
                df_clean = df.copy()
                df_clean[price_cols] = df_clean[price_cols].ffill()

            if not self.dry_run:
                self.backup_file(daily_path)
                df_clean.to_parquet(daily_path, index=False)
                logger.info(f"âœ… Daily NULL ì²˜ë¦¬ ì™„ë£Œ: {total_nulls}ê±´ ({strategy})")
            else:
                logger.info(f"  [DRY-RUN] NULL ì²˜ë¦¬ ì˜ˆì •: {total_nulls}ê±´")

            self.report["actions"].append(
                {
                    "type": "fill_nulls",
                    "file": str(daily_path),
                    "strategy": strategy,
                    "processed": int(total_nulls),
                }
            )

            return int(total_nulls)

        except Exception as e:
            logger.error(f"âŒ Daily NULL ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.report["errors"].append(
                {
                    "type": "fill_nulls",
                    "file": str(daily_path),
                    "error": str(e),
                }
            )
            return 0

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì „ì²´ ë³µêµ¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def repair_all(self, null_strategy: str = "ffill") -> dict:
        """
        ì „ì²´ ë°ì´í„° ë³µêµ¬ ì‹¤í–‰

        1. Daily ì¤‘ë³µ ì œê±°
        2. Intraday ì¤‘ë³µ ì œê±°
        3. Daily NULL ì²˜ë¦¬

        Args:
            null_strategy: NULL ì²˜ë¦¬ ì „ëµ

        Returns:
            dict: ë³µêµ¬ ë¦¬í¬íŠ¸
        """
        logger.info("=" * 60)
        logger.info("ğŸ”§ ë°ì´í„° ë³µêµ¬ ì‹œì‘")
        logger.info(f"   ëª¨ë“œ: {'DRY-RUN' if self.dry_run else 'APPLY'}")
        logger.info("=" * 60)

        # 1. ì¤‘ë³µ ì œê±°
        print("\nğŸ“Š ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°:")
        daily_dups = self.remove_duplicates_daily()
        intraday_dups = self.remove_duplicates_intraday()
        print(f"  Daily: {daily_dups}ê±´")
        print(f"  Intraday: {intraday_dups}ê±´")

        # 2. NULL ì²˜ë¦¬
        print("\nğŸ“Š NULL ê°’ ì²˜ë¦¬:")
        daily_nulls = self.fill_nulls_daily(strategy=null_strategy)
        print(f"  Daily: {daily_nulls}ê±´ ({null_strategy})")

        # ë¦¬í¬íŠ¸ ì™„ë£Œ
        self.report["completed_at"] = datetime.now().isoformat()
        self.report["summary"] = {
            "duplicates_removed": daily_dups + intraday_dups,
            "nulls_processed": daily_nulls,
            "total_actions": len(self.report["actions"]),
            "total_errors": len(self.report["errors"]),
        }

        logger.info("=" * 60)
        if self.dry_run:
            logger.info("ğŸ” DRY-RUN ì™„ë£Œ - ì‹¤ì œ ë³€ê²½ ì—†ìŒ")
        else:
            logger.info("âœ… ë°ì´í„° ë³µêµ¬ ì™„ë£Œ")
        logger.info("=" * 60)

        return self.report


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """CLI ì§„ì…ì """
    parser = argparse.ArgumentParser(description="Parquet ë°ì´í„° ë³µêµ¬ (11-004)")
    parser.add_argument(
        "--base-dir",
        default="data/parquet",
        help="Parquet ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: data/parquet)",
    )
    parser.add_argument(
        "--backup-dir",
        default="data/backup",
        help="ë°±ì—… ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: data/backup)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ìˆ˜ì • ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ (ê¸°ë³¸ ë™ì‘)",
    )
    parser.add_argument(
        "--apply",
        action="store_true",
        help="ì‹¤ì œë¡œ ë°ì´í„° ìˆ˜ì • ì ìš©",
    )
    parser.add_argument(
        "--null-strategy",
        choices=["ffill", "linear", "drop"],
        default="ffill",
        help="NULL ì²˜ë¦¬ ì „ëµ (ê¸°ë³¸: ffill)",
    )
    parser.add_argument(
        "--output-json",
        type=str,
        default=None,
        help="JSON ë¦¬í¬íŠ¸ ì¶œë ¥ ê²½ë¡œ",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥",
    )

    args = parser.parse_args()

    # dry_run ê²°ì •: --apply ì—†ìœ¼ë©´ ê¸°ë³¸ dry_run
    dry_run = not args.apply

    base_dir = Path(args.base_dir)
    if not base_dir.exists():
        logger.error(f"âŒ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        sys.exit(1)

    # ë³µêµ¬ ì‹¤í–‰
    repairer = DataRepairer(
        base_dir=base_dir,
        backup_dir=Path(args.backup_dir),
        dry_run=dry_run,
    )

    report = repairer.repair_all(null_strategy=args.null_strategy)

    # JSON ë¦¬í¬íŠ¸ ì €ì¥
    if args.output_json:
        output_path = Path(args.output_json)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ JSON ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    # ì¢…ë£Œ ì½”ë“œ
    return 0 if len(report["errors"]) == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
