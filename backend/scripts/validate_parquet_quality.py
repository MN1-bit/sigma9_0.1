# ============================================================================
# Parquet ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ìŠ¤í¬ë¦½íŠ¸ (11-003, 11-004)
# ============================================================================
# ğŸ“Œ ì´ íŒŒì¼ì˜ ì—­í• :
#   - Parquet íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
#   - í•„ìˆ˜ ì»´ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
#   - OHLC ê´€ê³„ ë¬´ê²°ì„± ê²€ì‚¬ (11-004)
#   - ì¤‘ë³µ ë ˆì½”ë“œ ê²€ì‚¬
#   - ë°ì´í„° ë²”ìœ„ ìœ íš¨ì„± ê²€ì¦
#   - JSON ë¦¬í¬íŠ¸ ì¶œë ¥ (11-004)
#
# ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
#   >>> python -m backend.scripts.validate_parquet_quality
#   >>> python -m backend.scripts.validate_parquet_quality --verbose
#   >>> python -m backend.scripts.validate_parquet_quality --output-json report.json
# ============================================================================

"""
Parquet ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬

[11-003] ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
[11-004] OHLC ê´€ê³„ ê²€ì¦, ê°­ íƒì§€, JSON ë¦¬í¬íŠ¸ ì¶”ê°€

ê²€ì‚¬ í•­ëª©:
1. íŒŒì¼ ì½ê¸° ê°€ëŠ¥ ì—¬ë¶€ (ë¬´ê²°ì„±)
2. í•„ìˆ˜ ì»´ëŸ¼ ì¡´ì¬ ì—¬ë¶€
3. OHLC ê´€ê³„ ë¬´ê²°ì„± (High >= max(O,C), Low <= min(O,C))
4. ì¤‘ë³µ ë ˆì½”ë“œ ê²€ì‚¬
5. NULL ê°’ ë¹„ìœ¨
6. ë°ì´í„° ë²”ìœ„ ìœ íš¨ì„±

ELI5: íŒŒì¼ë“¤ì´ ì œëŒ€ë¡œ ë˜ì–´ìˆëŠ”ì§€ ê±´ê°•ê²€ì§„ì„ í•©ë‹ˆë‹¤.
"""

import argparse
import json
import sys
from pathlib import Path
from collections import defaultdict
from datetime import datetime

import pyarrow.parquet as pq
from loguru import logger

# 11-004: ê²€ì¦ ëª¨ë“ˆ import
from backend.data.validators import (
    validate_ohlc_relationship,
    validate_volume,
    detect_daily_gaps,
    detect_price_outliers,
)


# í•„ìˆ˜ ì»¬ëŸ¼ ì •ì˜
DAILY_REQUIRED_COLS = ["ticker", "date", "open", "high", "low", "close", "volume"]
INTRADAY_REQUIRED_COLS = ["timestamp", "open", "high", "low", "close", "volume"]


def validate_daily(daily_dir: Path, verbose: bool = False) -> dict:
    """
    Daily Parquet í’ˆì§ˆ ê²€ì‚¬

    Args:
        daily_dir: daily ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥

    Returns:
        dict: ê²€ì‚¬ ê²°ê³¼
    """
    results = {
        "files": 0,
        "valid": 0,
        "errors": [],
        "warnings": [],
    }

    all_daily_path = daily_dir / "all_daily.parquet"

    if not all_daily_path.exists():
        results["errors"].append("all_daily.parquet íŒŒì¼ ì—†ìŒ")
        return results

    results["files"] = 1

    try:
        df = pq.read_table(all_daily_path).to_pandas()

        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
        missing = set(DAILY_REQUIRED_COLS) - set(df.columns)
        if missing:
            results["errors"].append(f"ëˆ„ë½ ì»¬ëŸ¼: {missing}")
            return results

        # ë°ì´í„° ì¡´ì¬ ê²€ì‚¬
        if len(df) == 0:
            results["errors"].append("ë¹ˆ íŒŒì¼")
            return results

        # ì¤‘ë³µ ê²€ì‚¬
        dups = df.duplicated(subset=["ticker", "date"]).sum()
        if dups > 0:
            results["warnings"].append(f"ì¤‘ë³µ ë ˆì½”ë“œ: {dups}ê±´")

        # NULL ê°’ ê²€ì‚¬
        null_counts = df[DAILY_REQUIRED_COLS].isnull().sum()
        null_cols = null_counts[null_counts > 0]
        if len(null_cols) > 0:
            results["warnings"].append(f"NULL ê°’ ë°œê²¬: {null_cols.to_dict()}")

        # í‹°ì»¤ ìˆ˜, ë‚ ì§œ ë²”ìœ„
        num_tickers = df["ticker"].nunique()
        date_range = f"{df['date'].min()} ~ {df['date'].max()}"

        if verbose:
            logger.info(f"  ğŸ“Š í‹°ì»¤ ìˆ˜: {num_tickers}")
            logger.info(f"  ğŸ“… ë‚ ì§œ ë²”ìœ„: {date_range}")
            logger.info(f"  ğŸ“ ë ˆì½”ë“œ ìˆ˜: {len(df)}")

        # [11-004] OHLC ê´€ê³„ ê²€ì¦
        ohlc_violations = validate_ohlc_relationship(df)
        if ohlc_violations:
            results["warnings"].append(f"OHLC ìœ„ë°˜: {len(ohlc_violations)}ê±´")
            results["ohlc_violations"] = ohlc_violations[:10]  # ìƒìœ„ 10ê±´ë§Œ ì €ì¥

        # [11-004] Volume ê²€ì¦ (ìŒìˆ˜)
        vol_violations = validate_volume(df)
        if vol_violations:
            results["warnings"].append(f"Volume ìŒìˆ˜: {len(vol_violations)}ê±´")

        # [11-004] Volume ëˆ„ë½ (OHLCëŠ” ìˆëŠ”ë° Volumeì´ 0 ë˜ëŠ” NULL)
        vol_missing_mask = ((df["volume"].isnull()) | (df["volume"] == 0)) & (
            df["close"] > 0
        )
        vol_missing_count = vol_missing_mask.sum()
        if vol_missing_count > 0:
            results["warnings"].append(f"Volume ëˆ„ë½(0/NULL): {vol_missing_count}ê±´")
            # ìƒ˜í”Œ 5ê°œ ì €ì¥
            sample_tickers = (
                df[vol_missing_mask].head(5)[["ticker", "date"]].to_dict("records")
            )
            results["volume_missing_samples"] = sample_tickers

        # [11-004] ë‚ ì§œ ê°­ ê²€ì‚¬ (í‹°ì»¤ë³„ ê±°ë˜ì¼ ëˆ„ë½)
        # ìƒìœ„ 100ê°œ í‹°ì»¤ë§Œ ìƒ˜í”Œ ê²€ì‚¬ (ì „ì²´ëŠ” ë„ˆë¬´ ëŠë¦¼)
        top_tickers = df["ticker"].value_counts().head(100).index.tolist()
        df_sample = df[df["ticker"].isin(top_tickers)]
        date_gaps = detect_daily_gaps(df_sample)
        total_gap_days = sum(len(v) for v in date_gaps.values())
        if total_gap_days > 0:
            results["warnings"].append(
                f"ë‚ ì§œ ê°­: {len(date_gaps)} í‹°ì»¤, {total_gap_days}ì¼"
            )
            # ìƒìœ„ 5ê°œ í‹°ì»¤ ê°­ ì €ì¥
            results["date_gaps_sample"] = {
                k: v[:5] for k, v in list(date_gaps.items())[:5]
            }

        # [11-004] ê°€ê²© ì´ìƒì¹˜ (Z-score > 3)
        # í‹°ì»¤ë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ê²€ì‚¬
        total_outliers = 0
        outlier_samples = []
        for ticker in top_tickers[:20]:  # ìƒìœ„ 20ê°œë§Œ
            ticker_df = df[df["ticker"] == ticker].sort_values("date")
            if len(ticker_df) < 10:
                continue
            outliers = detect_price_outliers(ticker_df, z_threshold=4.0)
            if outliers:
                total_outliers += len(outliers)
                for o in outliers[:2]:
                    o["ticker"] = ticker
                    outlier_samples.append(o)

        if total_outliers > 0:
            results["warnings"].append(f"ê°€ê²© ì´ìƒì¹˜: {total_outliers}ê±´ (z>4)")
            results["price_outliers_sample"] = outlier_samples[:10]

        results["valid"] = 1
        results["stats"] = {
            "tickers": num_tickers,
            "records": len(df),
            "date_range": date_range,
            "ohlc_violations": len(ohlc_violations),
            "volume_violations": len(vol_violations),
            "volume_missing": vol_missing_count,
            "date_gaps_tickers": len(date_gaps),
            "date_gaps_days": total_gap_days,
            "price_outliers": total_outliers,
        }

    except Exception as e:
        results["errors"].append(f"ì½ê¸° ì‹¤íŒ¨: {e}")

    return results


def _validate_single_intraday_file(
    file_path: Path,
    tf: str,
    full_ohlc: bool = False,
) -> dict:
    """
    ë‹¨ì¼ Intraday íŒŒì¼ ê²€ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼)

    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        tf: íƒ€ì„í”„ë ˆì„
        full_ohlc: OHLC ê´€ê³„ ê²€ì‚¬ í¬í•¨ ì—¬ë¶€

    Returns:
        dict: {valid, error, warning, ohlc_violations}
    """
    result = {
        "file": str(file_path),
        "tf": tf,
        "valid": False,
        "error": None,
        "warning": None,
        "ohlc_violations": 0,
        "records": 0,
    }

    try:
        df = pq.read_table(file_path).to_pandas()
        result["records"] = len(df)

        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
        missing = set(INTRADAY_REQUIRED_COLS) - set(df.columns)
        if missing:
            result["error"] = f"ëˆ„ë½ ì»¬ëŸ¼ {missing}"
            return result

        # ë¹ˆ íŒŒì¼ ê²€ì‚¬
        if len(df) == 0:
            result["error"] = "ë¹ˆ íŒŒì¼"
            return result

        # ì¤‘ë³µ ê²€ì‚¬
        dups = df.duplicated(subset=["timestamp"]).sum()
        if dups > 0:
            result["warning"] = f"ì¤‘ë³µ {dups}ê±´"

        # [11-004] OHLC ê´€ê³„ ê²€ì‚¬ (full ëª¨ë“œ)
        if full_ohlc:
            ohlc_violations = validate_ohlc_relationship(df)
            result["ohlc_violations"] = len(ohlc_violations)
            if ohlc_violations:
                result["warning"] = f"OHLC ìœ„ë°˜ {len(ohlc_violations)}ê±´"

        result["valid"] = True

    except Exception as e:
        result["error"] = f"ì½ê¸° ì‹¤íŒ¨ - {e}"

    return result


def validate_intraday(
    base_dir: Path,
    verbose: bool = False,
    full_ohlc: bool = False,
    sample_ratio: float = 1.0,
    max_workers: int = 4,
) -> dict:
    """
    Intraday Parquet í’ˆì§ˆ ê²€ì‚¬ (TFë³„ í´ë” êµ¬ì¡°)

    Args:
        base_dir: Parquet ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        full_ohlc: OHLC ê´€ê³„ ì‹¬ì¸µ ê²€ì‚¬ (ëŠë¦¼)
        sample_ratio: ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = 10%, 1.0 = ì „ì²´)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜

    Returns:
        dict: ê²€ì‚¬ ê²°ê³¼
    """
    import random
    from concurrent.futures import ThreadPoolExecutor, as_completed

    results = {
        "files": 0,
        "valid": 0,
        "errors": [],
        "warnings": [],
        "ohlc_violations_total": 0,
        "by_tf": defaultdict(
            lambda: {"files": 0, "valid": 0, "errors": 0, "ohlc_violations": 0}
        ),
        "mode": "full" if full_ohlc else "quick",
        "sample_ratio": sample_ratio,
    }

    # TFë³„ í´ë”ì—ì„œ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    tf_folders = ["1m", "3m", "5m", "15m", "1h", "4h"]
    all_files: list[tuple[Path, str]] = []  # (file_path, tf)

    for tf in tf_folders:
        tf_dir = base_dir / tf
        if not tf_dir.exists():
            continue
        for f in tf_dir.glob("*.parquet"):
            all_files.append((f, tf))

    # ìƒ˜í”Œë§
    total_files = len(all_files)
    if sample_ratio < 1.0:
        sample_size = max(1, int(total_files * sample_ratio))
        all_files = random.sample(all_files, sample_size)
        logger.info(
            f"ğŸ² ìƒ˜í”Œë§: {total_files} â†’ {len(all_files)}ê°œ ({sample_ratio * 100:.0f}%)"
        )

    results["files"] = len(all_files)

    # ë³‘ë ¬ ê²€ì‚¬
    logger.info(
        f"ğŸ” {len(all_files)}ê°œ íŒŒì¼ ê²€ì‚¬ ì‹œì‘ (workers={max_workers}, full_ohlc={full_ohlc})"
    )

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(_validate_single_intraday_file, f, tf, full_ohlc): (f, tf)
            for f, tf in all_files
        }

        done_count = 0
        for future in as_completed(futures):
            done_count += 1
            if verbose and done_count % 1000 == 0:
                logger.info(f"  ì§„í–‰: {done_count}/{len(all_files)}")

            result = future.result()
            tf = result["tf"]
            results["by_tf"][tf]["files"] += 1

            if result["valid"]:
                results["valid"] += 1
                results["by_tf"][tf]["valid"] += 1
            if result["error"]:
                results["errors"].append(
                    f"{tf}/{Path(result['file']).name}: {result['error']}"
                )
                results["by_tf"][tf]["errors"] += 1
            if result["warning"]:
                results["warnings"].append(
                    f"{tf}/{Path(result['file']).name}: {result['warning']}"
                )
            if result["ohlc_violations"] > 0:
                results["ohlc_violations_total"] += result["ohlc_violations"]
                results["by_tf"][tf]["ohlc_violations"] = (
                    results["by_tf"][tf].get("ohlc_violations", 0)
                    + result["ohlc_violations"]
                )

    logger.info(f"âœ… ê²€ì‚¬ ì™„ë£Œ: {results['valid']}/{results['files']} ì •ìƒ")

    return results


def main():
    """CLI ì§„ì…ì """
    parser = argparse.ArgumentParser(
        description="Parquet ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ (11-003, 11-004)"
    )
    parser.add_argument(
        "--base-dir",
        default="data/parquet",
        help="Parquet ë² ì´ìŠ¤ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: data/parquet)",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥",
    )
    parser.add_argument(
        "--output-json",
        type=str,
        default=None,
        help="JSON ë¦¬í¬íŠ¸ ì¶œë ¥ ê²½ë¡œ (ì˜ˆ: data/reports/integrity.json)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Intraday OHLC ì‹¬ì¸µ ê²€ì‚¬ í¬í•¨ (ëŠë¦¼, ~10ë¶„)",
    )
    parser.add_argument(
        "--sample",
        type=float,
        default=1.0,
        help="ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = 10%%, 1.0 = ì „ì²´, ê¸°ë³¸: 1.0)",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 4)",
    )

    args = parser.parse_args()

    base_dir = Path(args.base_dir)

    if not base_dir.exists():
        logger.error(f"âŒ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        sys.exit(1)

    print("=" * 60)
    print("Parquet ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ (11-003)")
    print("=" * 60)

    total_errors = 0

    # Daily ê²€ì‚¬
    print("\nğŸ“Š Daily ë°ì´í„° ê²€ì‚¬:")
    daily_results = validate_daily(base_dir / "daily", verbose=args.verbose)
    print(f"  íŒŒì¼ ìˆ˜: {daily_results['files']}")
    print(f"  ì •ìƒ: {daily_results['valid']}")
    print(f"  ì˜¤ë¥˜: {len(daily_results['errors'])}")
    print(f"  ê²½ê³ : {len(daily_results['warnings'])}")

    if daily_results.get("stats"):
        stats = daily_results["stats"]
        print(f"  â”œâ”€ í‹°ì»¤: {stats['tickers']}ê°œ")
        print(f"  â”œâ”€ ë ˆì½”ë“œ: {stats['records']:,}ê°œ")
        print(f"  â””â”€ ë‚ ì§œ: {stats['date_range']}")

    for err in daily_results["errors"][:3]:
        print(f"    â›” {err}")
    for warn in daily_results["warnings"][:3]:
        print(f"    âš ï¸ {warn}")

    total_errors += len(daily_results["errors"])

    # Intraday ê²€ì‚¬
    mode_str = "ì‹¬ì¸µ(OHLC)" if args.full else "ë¹ ë¥¸"
    sample_str = f" (ìƒ˜í”Œ {args.sample * 100:.0f}%)" if args.sample < 1.0 else ""
    print(f"\nğŸ“Š Intraday ë°ì´í„° ê²€ì‚¬ ({mode_str}{sample_str}):")

    intraday_results = validate_intraday(
        base_dir,
        verbose=args.verbose,
        full_ohlc=args.full,
        sample_ratio=args.sample,
        max_workers=args.workers,
    )
    print(f"  íŒŒì¼ ìˆ˜: {intraday_results['files']}")
    print(f"  ì •ìƒ: {intraday_results['valid']}")
    print(f"  ì˜¤ë¥˜: {len(intraday_results['errors'])}")
    print(f"  ê²½ê³ : {len(intraday_results['warnings'])}")
    if args.full and intraday_results.get("ohlc_violations_total", 0) > 0:
        print(f"  OHLC ìœ„ë°˜: {intraday_results['ohlc_violations_total']}ê±´")

    if intraday_results["by_tf"]:
        print("\n  íƒ€ì„í”„ë ˆì„ë³„:")
        for tf, stats in sorted(intraday_results["by_tf"].items()):
            if stats["files"] > 0:
                valid_pct = (
                    (stats["valid"] / stats["files"] * 100) if stats["files"] else 0
                )
                print(f"    {tf}: {stats['files']} íŒŒì¼ ({valid_pct:.0f}% ì •ìƒ)")

    for err in intraday_results["errors"][:5]:
        print(f"    â›” {err}")

    total_errors += len(intraday_results["errors"])

    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    if total_errors == 0:
        print("âœ… ëª¨ë“  ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í†µê³¼!")
    else:
        print(f"âš ï¸ ì´ {total_errors}ê±´ì˜ ì˜¤ë¥˜ ë°œê²¬")

    # [11-004] JSON ë¦¬í¬íŠ¸ ì¶œë ¥
    if args.output_json:
        report = {
            "generated_at": datetime.now().isoformat(),
            "base_dir": str(base_dir),
            "daily": daily_results,
            "intraday": {
                k: v
                for k, v in intraday_results.items()
                if k != "by_tf"  # defaultdict ì§ë ¬í™” ë¬¸ì œ íšŒí”¼
            },
            "intraday_by_tf": dict(intraday_results.get("by_tf", {})),
            "total_errors": total_errors,
            "passed": total_errors == 0,
        }

        output_path = Path(args.output_json)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ“„ JSON ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    return 0 if total_errors == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
