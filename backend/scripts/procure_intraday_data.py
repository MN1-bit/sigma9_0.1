# ============================================================================
# Intraday Data Procurement Script
# ============================================================================
# ğŸ“Œ ì´ íŒŒì¼ì˜ ì—­í• :
#   - 8,000 ì¢…ëª©ì˜ 1ë¶„ë´‰/1ì‹œê°„ë´‰ ë°ì´í„°ë¥¼ Massive APIë¡œ ì¡°ë‹¬
#   - ì‹¤ì‹œê°„ Parquet ë³€í™˜ ë° ì €ì¥
#   - ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ë¡œê¹… ë° ì¤‘ë‹¨ ì‹œ ì¬ê°œ ì§€ì›
#
# ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
#   >>> python -m backend.scripts.procure_intraday_data
#   >>> python -m backend.scripts.procure_intraday_data --test  # 10ê°œë§Œ í…ŒìŠ¤íŠ¸
# ============================================================================

import asyncio
import sys
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
from loguru import logger

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ (API í‚¤ ë“±)
from dotenv import load_dotenv

load_dotenv(project_root / ".env")

from backend.data.massive_client import MassiveClient
from backend.data.parquet_manager import ParquetManager
from backend.data.database import MarketDB


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì¡°ë‹¬ ë²”ìœ„
DAYS_1M = 10  # 1ë¶„ë´‰: 10 ê±°ë˜ì¼
DAYS_1H = 63  # 1ì‹œê°„ë´‰: 3ê°œì›” (~63 ê±°ë˜ì¼)

# ê²½ë¡œ
DB_PATH = "data/market_data.db"
PARQUET_DIR = "data/parquet"
PROGRESS_FILE = "data/procurement_progress.json"

# Rate Limit ì„¤ì • (100 req/min = 0.6ì´ˆ/í˜¸ì¶œ)
REQUEST_DELAY = 0.6


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì§„í–‰ ìƒí™© ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_progress() -> set:
    """ì™„ë£Œëœ í‹°ì»¤ ëª©ë¡ ë¡œë“œ (ì¬ê°œ ì§€ì›)"""
    import json

    progress_path = Path(PROGRESS_FILE)
    if progress_path.exists():
        with open(progress_path, "r") as f:
            data = json.load(f)
            return set(data.get("completed", []))
    return set()


def save_progress(completed: set):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    import json

    progress_path = Path(PROGRESS_FILE)
    progress_path.parent.mkdir(parents=True, exist_ok=True)
    with open(progress_path, "w") as f:
        json.dump(
            {"completed": list(completed), "last_updated": datetime.now().isoformat()},
            f,
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì¡°ë‹¬ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def procure_intraday_data(test_mode: bool = False):
    """
    8,000 ì¢…ëª©ì˜ 1m/1h ë°ì´í„° ì¡°ë‹¬

    Args:
        test_mode: Trueì´ë©´ 10ê°œ ì¢…ëª©ë§Œ í…ŒìŠ¤íŠ¸
    """
    logger.info("=" * 60)
    logger.info("ğŸ“¥ Intraday Data Procurement ì‹œì‘")
    logger.info("=" * 60)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì´ˆê¸°í™”
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    db = MarketDB(DB_PATH)
    await db.initialize()

    pm = ParquetManager(PARQUET_DIR)

    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    end_date = datetime.now()
    start_date_1m = end_date - timedelta(days=DAYS_1M + 5)  # ì—¬ìœ ë¶„ ì¶”ê°€
    start_date_1h = end_date - timedelta(days=DAYS_1H + 10)

    from_1m = start_date_1m.strftime("%Y-%m-%d")
    from_1h = start_date_1h.strftime("%Y-%m-%d")
    to_date = end_date.strftime("%Y-%m-%d")

    logger.info(f"ğŸ“… 1ë¶„ë´‰ ë²”ìœ„: {from_1m} ~ {to_date} ({DAYS_1M}ì¼)")
    logger.info(f"ğŸ“… 1ì‹œê°„ë´‰ ë²”ìœ„: {from_1h} ~ {to_date} ({DAYS_1H}ì¼)")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í‹°ì»¤ ëª©ë¡ ì¡°íšŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tickers = await db.get_all_tickers_with_data()

    if not tickers:
        logger.warning("âš ï¸ DBì— í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¼ë´‰ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
        return

    logger.info(f"ğŸ“Š ì´ {len(tickers)}ê°œ í‹°ì»¤ ë°œê²¬")

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if test_mode:
        tickers = tickers[:10]
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(tickers)}ê°œë§Œ ì²˜ë¦¬")

    # ì§„í–‰ ìƒí™© ë¡œë“œ (ì¬ê°œ ì§€ì›)
    completed = load_progress()
    remaining = [t for t in tickers if t not in completed]

    if completed:
        logger.info(
            f"ğŸ“Œ ì´ì „ ì§„í–‰ ë³µì›: {len(completed)}ê°œ ì™„ë£Œ, {len(remaining)}ê°œ ë‚¨ìŒ"
        )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # API í´ë¼ì´ì–¸íŠ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import os

    api_key = os.environ.get("MASSIVE_API_KEY")
    if not api_key:
        logger.error("âŒ MASSIVE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    async with MassiveClient(api_key=api_key) as client:
        total = len(remaining)
        success = 0
        errors = 0
        start_time = datetime.now()

        for i, ticker in enumerate(remaining):
            try:
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 1ë¶„ë´‰ ì¡°ë‹¬
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                bars_1m = await client.fetch_intraday_bars(
                    ticker=ticker,
                    multiplier=1,
                    from_date=from_1m,
                    to_date=to_date,
                    limit=5000,
                )

                if bars_1m:
                    df_1m = pd.DataFrame(bars_1m)
                    pm.append_intraday(ticker, "1m", df_1m)

                await asyncio.sleep(REQUEST_DELAY)

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 1ì‹œê°„ë´‰ ì¡°ë‹¬
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                bars_1h = await client.fetch_intraday_bars(
                    ticker=ticker,
                    multiplier=60,
                    from_date=from_1h,
                    to_date=to_date,
                    limit=5000,
                )

                if bars_1h:
                    df_1h = pd.DataFrame(bars_1h)
                    pm.append_intraday(ticker, "1h", df_1h)

                await asyncio.sleep(REQUEST_DELAY)

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                success += 1
                completed.add(ticker)

                # 100ê°œë§ˆë‹¤ ì €ì¥ ë° ë¡œê·¸
                if (i + 1) % 100 == 0:
                    save_progress(completed)
                    elapsed = (datetime.now() - start_time).total_seconds()
                    rate = (i + 1) / elapsed if elapsed > 0 else 0
                    eta = (total - i - 1) / rate if rate > 0 else 0

                    logger.info(
                        f"ğŸ“Š ì§„í–‰: {i + 1}/{total} ({(i + 1) / total * 100:.1f}%) "
                        f"| ì„±ê³µ: {success} | ì˜¤ë¥˜: {errors} "
                        f"| ETA: {eta / 60:.1f}ë¶„"
                    )

            except Exception as e:
                errors += 1
                logger.warning(f"âš ï¸ {ticker} ì‹¤íŒ¨: {e}")

                # ì—°ì† ì˜¤ë¥˜ 5íšŒ ì‹œ ì¤‘ë‹¨
                if errors > 5:
                    logger.error("ğŸ›‘ ì˜¤ë¥˜ê°€ ë„ˆë¬´ ë§ì•„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    save_progress(completed)
                    break

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì™„ë£Œ ë³´ê³ 
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    save_progress(completed)
    elapsed = (datetime.now() - start_time).total_seconds() / 60

    logger.info("=" * 60)
    logger.info("âœ… Procurement ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì„±ê³µ: {success}/{total} ì¢…ëª©")
    logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ë¶„")
    logger.info("=" * 60)

    # ìµœì¢… í†µê³„
    stats = pm.get_stats()
    logger.info(f"ğŸ“¦ Parquet í†µê³„: {stats}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Intraday Data Procurement")
    parser.add_argument(
        "--test", action="store_true", help="Test mode (10 tickers only)"
    )
    parser.add_argument(
        "--reset", action="store_true", help="Reset progress and start fresh"
    )
    args = parser.parse_args()

    if args.reset:
        progress_path = Path(PROGRESS_FILE)
        if progress_path.exists():
            progress_path.unlink()
            logger.info("ğŸ—‘ï¸ Progress reset")

    asyncio.run(procure_intraday_data(test_mode=args.test))
